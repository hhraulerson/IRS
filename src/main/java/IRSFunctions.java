/*
 * File:    IRSFunctions.java
 * Created: August 10, 2017
 * Author:  hhraulerson
 * Project: Irrigation Recommendation System (IRS)
 */

import org.deeplearning4j.nn.conf.GradientNormalization;
import org.deeplearning4j.nn.conf.BackpropType;
import org.deeplearning4j.nn.conf.MultiLayerConfiguration;
import org.deeplearning4j.nn.conf.NeuralNetConfiguration;
import org.deeplearning4j.nn.conf.Updater;
import org.deeplearning4j.nn.conf.layers.GravesLSTM;
import org.deeplearning4j.nn.conf.layers.RnnOutputLayer;
import org.deeplearning4j.nn.multilayer.MultiLayerNetwork;
import org.deeplearning4j.eval.RegressionEvaluation;
import org.deeplearning4j.nn.api.Layer;
import org.deeplearning4j.nn.api.OptimizationAlgorithm;
import org.deeplearning4j.nn.weights.WeightInit;
import org.deeplearning4j.optimize.listeners.ScoreIterationListener;
import org.deeplearning4j.util.ModelSerializer;
import org.nd4j.linalg.activations.Activation;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.dataset.DataSet;
import org.nd4j.linalg.lossfunctions.LossFunctions;
import java.io.File;
import java.io.IOException;

/**
 * <p>
 * The IRSFunctions class contains methods for configuring and building
 * the IRS's recurrent neural network (RNN)/model.
 * </p>
 *
 * @author hhraulerson
 * @version 1.7
 */
public class IRSFunctions
{
    /* Variables */

    /**
     * The IRSIterator, which inherits from the DataSetIterator class,
     * that will iterate through a given dataset.
     */
    private IRSIterator iter;

    /**
     * The MultiLayerConfiguration that contains the hyperparameters
     * for the RNN.
     */
    private MultiLayerConfiguration RNNconfiguration;

    /**
     * The MultiLayerNetwork that is the RNN.
     */
    private MultiLayerNetwork rnn;

    /**
     * The variable that contains the number of output layers.
     */
    private int outputLayers;

    /**
     * The variable that stores the number of nodes in the LSTM layer.
     */
    private int LSTMlayers;

    /**
     * The variable that stores the minibatch size to use when training.
     */
    private int miniBatch;

    /**
     * The variable that stores the training example set size to use.
     */
    private int exampleSize;

    /**
     * The variable that stores the size for truncated backpropagation through
     * time (tbptt).
     */
    private int tbpttSize;

    /**
     * The variable that stores the number of training epochs.
     */
    private int epochs;

    /**
     * The variable that stores the crop type specified by the user.
     */
    private String crop;

    /**
     * The variable that contains the soil type specified by the user.
     */
    private String soil;

    /**
     * The variable that contains the file path provided by the user.
     */
    private String dataFilePath;

    /**
     * The Recommendation object used to create a recommendation
     * report.
     */
    private Recommendation report;

    /**
     * The string variable that contains the results of the recommendation
     * generated by the system.
     */
    private String results;

    /**
     * The variable that stores the location of the file containing
     * the parameters which are saved after the model is run (if the
     * user chooses to save the parameters!).
     */
    private String savedParamPath;

    /* Constructors */

    /**
     * Default constructor for IRSFunctions class.
     * @param layerSize number of nodes in the LSTM layer
     * @param miniBatchs number of miniBatchs
     * @param exSize the total number of examples to iterate over
     * @param tbpttsize the number of examples to propagate forwards and backwards through
     * @param numEpochs the total number of epochs (times to go through the data)
     * @param dataFilePath the path of the file selected by the user
     * @param numColumns the number of columns for the file that was uploaded/selected by the user
     */
    public IRSFunctions(int layerSize, int miniBatchs, int exSize, int tbpttsize, int numEpochs, String dataFilePath, int numColumns)
    {
        setLSTMLayers(layerSize);
        setMiniBatches(miniBatchs);
        setExamples(exSize);
        setTbpttSize(tbpttsize);
        setEpochs(numEpochs);

        //create new IRSIterator
        iter = new IRSIterator(dataFilePath, numColumns, getMiniBatches(), getExamples());

        setOutputLayers(iter.totalOutcomes());

        //configure the RNN
        try
        {
            RNNconfiguration = configureRNN();
        }
        catch (Exception e)
        {
            //print to console - this isn't a message to be displayed to the user
            System.err.println("Caught Exception: RNN couldn't be configured.");
        }

        //creates the RNN
        createRNN(RNNconfiguration);
    }

    /**
     * Special constructor for IRSFunctions class - only called when RNN is
     * loaded from a previous model run.
     * @param loadLocation the file that will be used to load the RNN
     */
    public IRSFunctions(File loadLocation)
    {
        //load model from saved parameters
        try
        {
            rnn = ModelSerializer.restoreMultiLayerNetwork(loadLocation);
        }
        catch (Exception e)
        {
            //print to console - this isn't a message to be displayed to the user
            System.err.println("Caught Exception: RNN couldn't be loaded from parameters file.");

            //rnn = null;
        }

        setLSTMLayers(500);
        setMiniBatches(1);
        setExamples(250);
        setTbpttSize(250);
        setEpochs(1);
    }


    /**
     * Sets up the hyperparameters for the RNN.
     * @return returns a MultiLayerConfiguration object
     */
    public MultiLayerConfiguration configureRNN()
    {
            //set RNN hyperparameters
            RNNconfiguration = new NeuralNetConfiguration.Builder()
                    .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).iterations(1)
                    .gradientNormalization(GradientNormalization.RenormalizeL2PerLayer)
                    .learningRate(0.001)
                    .seed(12345)
                    .regularization(true)
                    .l2(0.0001)
                    .weightInit(WeightInit.XAVIER)
                    .updater(Updater.RMSPROP)
                    .list()
                    .layer(0, new GravesLSTM.Builder().nIn(iter.inputColumns()).nOut(getLSTMLayers())
                            .activation(Activation.SOFTSIGN).build())
                    .layer(1, new GravesLSTM.Builder().nIn(getLSTMLayers()).nOut(getLSTMLayers())
                            .activation(Activation.SOFTSIGN).build())
                    .layer(2, new RnnOutputLayer.Builder(LossFunctions.LossFunction.MSE).activation(Activation.IDENTITY)
                            .nIn(getLSTMLayers()).nOut(getOutputLayers()).build())
                    .backpropType(BackpropType.TruncatedBPTT).tBPTTForwardLength(getTbpttSize()).tBPTTBackwardLength(getTbpttSize())
                    .pretrain(false).backprop(true)
                    .build();

            return RNNconfiguration;
    }

    /**
     * Builds the RNN using the MultiLayerConfiguration variable.
     * @param conf the configuration (hyperparameters) of the RNN
     */
    public void createRNN(MultiLayerConfiguration conf)
    {
        rnn = new MultiLayerNetwork(conf);

        //initializes the RNN - this should be called once before the network is used.
        rnn.init();

        //value of the loss function on the current minibatch (want it close to 0!)
        rnn.setListeners(new ScoreIterationListener(1));

        //print the number of parameters in the network (and for each layer)
        Layer[] layers = rnn.getLayers();

        int totalNumParams = 0;

        for (int i = 0; i < layers.length; ++i)
        {
            int nParams = layers[i].numParams();

            //print to console - this isn't a message to be displayed to the user
            System.out.println("Number of parameters in layer " + i + ": " + nParams);

            totalNumParams += nParams;
        }

        //print to console - this isn't a message to be displayed to the user
        System.out.println("RNN created successfully!");
    }

    /**
     * Sets the LSTMlayers variable
     * @param nodes the size of the LSTM layer
     */
    public void setLSTMLayers(int nodes)
    {
        if (nodes <= 0)
        {
            //print to console - this isn't a message to be displayed to the user
            System.out.println("\nLTSM layer size provided is <= 0; LTSM is being set to " +
                    "default size of 500.\n");

            LSTMlayers = 500;
        }
        else
        {
            LSTMlayers = nodes;
        }
    }

    /**
     * Returns the LTSMlayers variable.
     * @return the number of nodes in the LSTM layer
     */
    public int getLSTMLayers()
    {
        return LSTMlayers;
    }

    /**
     * Sets the outputLayers variable.
     * @param out the number of output nodes for the RNN
     */
    public void setOutputLayers(int out)
    {
        if (out <= 0)
        {
            //print to console - this isn't a message to be displayed to the user
            System.out.println("\nNumber of output nodes provided is <= 0; Number of output nodes is " +
                    "being set to default size of 1.\n");

            outputLayers = 1;
        }
        else
        {
            outputLayers = out;
        }
    }

    /**
     * Returns the outputLayers variable.
     * @return the number of output nodes for the RNN
     */
    public int getOutputLayers()
    {
        return outputLayers;
    }

    /**
     * Sets the miniBatch variable.
     * @param miniBatches the number of minibatchs for the RNN to be trained on
     */
    public void setMiniBatches(int miniBatches)
    {

        if (miniBatches <= 0)
        {
            //print to console - this isn't a message to be displayed to the user
            System.out.println("\nMini batch size provided is <= 0; mini batch size is " +
                    "being set to default size of 1.\n");

            miniBatch = 1;
        }
        else
        {
            miniBatch = miniBatches;
        }
    }

    /**
     * Returns the miniBatch variable.
     * @return the number of minibatches to train the RNN with
     */
    public int getMiniBatches()
    {
        return miniBatch;
    }

    /**
     * Sets the exampleSize variable.
     * @param ex the number of examples for the RNN to be trained on
     */
    public void setExamples(int ex)
    {
        if (ex <= 0)
        {
            //print to console - this isn't a message to be displayed to the user
            System.out.println("\nExample length provided is <= 0; example length is " +
                    "being set to default size of 250.\n");

            exampleSize = 250;
        }
        else
        {
            exampleSize = ex;
        }
    }

    /**
     * Returns the exampleSize variable.
     * @return the number of examples to train the RNN with
     */
    public int getExamples()
    {
        return exampleSize;
    }

    /**
     * Sets the tbpttSize variable.
     * @param size the number of examples for the RNN to propagate forward and backward
     * through
     */
    public void setTbpttSize(int size)
    {
        if (size <= 0)
        {
            //print to console - this isn't a message to be displayed to the user
            System.out.println("\nTBTT length provided is <= 0; TBTT length is " +
                    "being set to default size of 100.\n");

            tbpttSize = 100;
        }
        else
        {
            tbpttSize = size;
        }
    }

    /**
     * Returns the tbpttSize variable.
     * @return the number of examples for the RNN to propagate forward and backward
     * through
     */
    public int getTbpttSize()
    {
        return tbpttSize;
    }

    /**
     * Sets the epochs variable.
     * @param numEpochs the number of epochs for the RNN to be trained on
     */
    public void setEpochs(int numEpochs)
    {
        if (numEpochs <= 0)
        {
            //print to console - this isn't a message to be displayed to the user
            System.out.println("\nNumber of epochs provided is <= 0; Number of epochs is " +
                    "being set to default size of 10.\n");

            epochs = 10;
        }
        else
        {
            epochs = numEpochs;
        }
    }

    /**
     * Returns the epochs variable.
     * @return the number of epochs set for the RNN
     */
    public int getEpochs()
    {
        return epochs;
    }

    /**
     * Sets the crop variable.
     * @param cropType the crop type
     */
    public void setCropType(String cropType)
    {
        crop = cropType;
    }

    /**
     * Returns the crop variable.
     * @return the crop type
     */
    public String getCropType()
    {
        return crop;
    }

    /**
     * Sets the soil variable.
     * @param soilType the soil type
     */
    public void setSoilType(String soilType)
    {
        soil = soilType;
    }

    /**
     * Returns the soil variable.
     * @return the soil type
     */
    public String getSoilType()
    {
        return soil;
    }

    /**
     * Sets the dataFilePath variable.
     * @param path the file path provided by the user
     */
    public void setDataFilePath(String path)
    {
        dataFilePath =  path;
    }

    /**
     * Returns the dataFilePath variable.
     * @return the file path provided by the user
     */
    public String getDataFilePath()
    {
        return dataFilePath;
    }


    /**
     * Sets the savedParamPath variable.
     * @param fileName the file path provided by the system
     */
    public void setSavedParametersFile(String fileName)
    {
        savedParamPath = fileName;
    }

    /**
     * Returns the path and name for the
     * file storing the saved parameters from
     * the model run.
     * @return the file path (including name)
     */
    public String getSavedParametersFile()
    {
        return savedParamPath;
    }

    /**
     * Trains/fits the data to the RNN.
     * @param save whether to save the parameters or not
     * @return a RegressionEvaluation object (used to print stats for the RNN)
     */
    public RegressionEvaluation runModel(int save)
    {
        DataSet ds = null;

        for (int i = 0; i < getEpochs(); ++i)
        {
            //print to console - this isn't a message to be displayed to the user
            System.out.println("Epoch " + i);

            iter.reset();

            //if file has another dataset, grab it and train data on it
            while (iter.hasNext())
            {
                ds = iter.next();
                rnn.fit(ds);
            }
        }

        INDArray features;
        INDArray labels;
        INDArray predicted;

        features = ds.getFeatureMatrix();
        labels = ds.getLabels();
        predicted = rnn.output(features, true);

        rnn.rnnClearPreviousState();

        iter.reset();

        if(save == 0)
        {
            String cwd;

            try
            {
                //get current working directory
                cwd = new java.io.File(".").getCanonicalPath();

                setSavedParametersFile(cwd + "/" + getCropType() + getSoilType() + "Params.zip");

                //location to save the network's parameters (.zip file)
                File locationToSave = new File(getSavedParametersFile());
                boolean updater = false;

                //write model to .zip file
                ModelSerializer.writeModel(rnn, locationToSave, updater);
            }
            catch(IOException e)
            {
                //print to console - this isn't a message to be displayed to the user
                System.err.println("Caught IOException: RNN couldn't be written to file.");

                return null;
            }
        }

        RegressionEvaluation evaluation = new RegressionEvaluation(1);
        evaluation.evalTimeSeries(labels, predicted);

        return evaluation;
    }

    /**
     * Creates the Recommendation report.
     * @param numColumns the number of columns in the uploaded file
     * @param dep1 depth of sensor #1
     * @param dep2 depth of sensor #2
     * @param dep3 depth of sensor #3
     * @return the file path for the Recommendation report
     */
    public String generateRecommendation(int numColumns, double dep1, double dep2, double dep3)
    {
        iter = new IRSIterator(getDataFilePath(), numColumns, getMiniBatches(), getExamples());

        if(iter == null)
        {
            return null;
        }
        else
        {

            iter.reset();
            DataSet ds = iter.next();

            int index = ds.getLabels().getColumn(0).length() - 1;
            rnn.rnnClearPreviousState();

            //allows forward pass (predictions) to be conducted efficiently, one or more steps at a time.
            INDArray out = rnn.rnnTimeStep(ds.getFeatures());

            //print to console - this isn't a message to be displayed to the user
            System.out.println("Calculated result: " + out.getRow(0).getDouble(index));
            System.out.println("Observed result: " + ds.getLabels().getColumn(0).getDouble(index));

            //create Recommendation object
            report = new Recommendation(getCropType(), getSoilType());

            report.setSensorDepth(1, dep1);
            report.setSensorDepth(2, dep2);
            report.setSensorDepth(3, dep3);

            setResults(out.getRow(0).getDouble(index));

            //creates the Recommendation report and stores the report's filepath
            String reportPath = report.createReport(getResults());

            //returns the report's file path
            return reportPath;
        }
    }

    /**
     * Sets the results variable.
     * @param amount the irrigation recommendation
     * provided by the RNN based on data provided
     * by the user
     */
    public void setResults(double amount)
    {
        String amt = String.format("%.2f", amount);

        if(amount <= 0)
        {
            results = "";
        }
        else
        {
            results = amt;
        }

    }

    /**
     * Returns the results variable.
     * @return the results of the RNN's recommendation
     */
    public String getResults()
    {
        return results;
    }

    /**
     * Returns the RNN.
     * @return the the RNN
     */
    public MultiLayerNetwork getNetwork()
    {
        return rnn;
    }

}
